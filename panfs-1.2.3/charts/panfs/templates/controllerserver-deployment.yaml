{{/* 
  # Copyright 2025 VDURA Inc.
  #
  # Licensed under the Apache License, Version 2.0 (the "License");
  # you may not use this file except in compliance with the License.
  # You may obtain a copy of the License at
  #
  #     http://www.apache.org/licenses/LICENSE-2.0
  #
  # Unless required by applicable law or agreed to in writing, software
  # distributed under the License is distributed on an "AS IS" BASIS,
  # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
  # See the License for the specific language governing permissions and
  # limitations under the License.
*/}}
# SPDX-License-Identifier: Apache-2.0
# Copyright 2025 VDURA Inc.

# Deployment for the CSI PanFS controller server
apiVersion: apps/v1
kind: Deployment
metadata:
  name: csi-panfs-controller
  namespace: {{ .Release.Namespace }}
  labels:
    app: csi-panfs-controller
    product: com.vdura.csi.panfs
    {{- if .Values.labels }}
    {{- toYaml .Values.labels | nindent 4 }}
    {{- end }}

spec: # csi-panfs-controller

  # Number of controller replicas
  replicas: {{ .Values.controllerServer.replicaCount }}

  # Update strategy for the controller deployment
  strategy:
    type: {{ .Values.controllerServer.strategy.type }}
    rollingUpdate:
      maxUnavailable: {{ .Values.controllerServer.strategy.rollingUpdate.maxUnavailable }}
      maxSurge: {{ .Values.controllerServer.strategy.rollingUpdate.maxSurge }}

  selector:
    matchLabels:
      app: csi-panfs-controller

  template:
    metadata:
      labels:
        app: csi-panfs-controller
        product: com.vdura.csi.panfs
        {{- if .Values.labels }}
        {{- toYaml .Values.labels | nindent 8 }}
        {{- end }}
    spec:
      {{- if .Values.controllerServer.affinity }}

      # Affinity settings for the controller pods
      affinity:
        {{- toYaml .Values.controllerServer.affinity | nindent 8 }}
      {{- end }}

      # Priority class for the controller pods
      priorityClassName: {{ .Values.controllerServer.priorityClassName }}

      # ServiceAccount for the controller pods
      serviceAccount: {{ .Release.Namespace }}-controller

      {{- if .Values.imagePullSecrets }}

      # Please make sure you have created the image pull secret in the same namespace
      # Check with: kubectl get secrets -n {{ .Release.Namespace }}
      # If not, create with:
      # kubectl create secret docker-registry -n {{ .Release.Namespace }} {{ index .Values.imagePullSecrets 0 }} \
      #   --docker-server=<REGISTRY_URL> \
      #   --docker-username=<USERNAME> \
      #   --docker-password=<PASSWORD_OR_TOKEN>
      imagePullSecrets:
        {{- range .Values.imagePullSecrets }}
        - name: {{ . }}
        {{- end }}
      {{- else }}

      # Uncomment the line below and set the secret name to enable pulling images from a private registry.
      imagePullSecrets:
        # - name: <IMAGE_PULL_SECRET_NAME>
      {{- end }}
      {{- if .Values.controllerServer.tolerations}}

      # Tolerations for scheduling the Deployment pods
      # Should match the tolerations of the PanFS consumer workloads
      # Double-check that the tolerations are correct:
      # kubectl describe node <NODE_NAME> | grep -A10 Taints
      tolerations:
        {{- toYaml .Values.controllerServer.tolerations | nindent 8 }}
      {{- end }}

      containers:
        # Main PanFS CSI plugin container
        - name: csi-panfs-plugin
          image: {{ .Values.csi.image | replace "{{ .Chart.AppVersion }}" .Chart.AppVersion }}
          imagePullPolicy: {{ .Values.csi.pullPolicy }}
          args:
            - "--endpoint=$(CSI_ENDPOINT)"
            - "--v={{ .Values.csi.logLevel }}"
          env:
            - name: CSI_ENDPOINT
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          {{- if .Values.csi.resources }}

          # Resource requests and limits for the driver main container
          # Adjust as necessary based on your cluster capacity and requirements
          # Limits should be set to prevent excessive resource consumption
          # Requests should be set to ensure the container gets scheduled
          resources:
            {{- toYaml .Values.csi.resources | nindent 12 }}
          {{- end }}

          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy

          livenessProbe:
            exec:
              command: ["sh", "-c", "test -S $(CSI_ENDPOINT)"]
            initialDelaySeconds: 10
            periodSeconds: 20
            timeoutSeconds: 5
            successThreshold: 1
            failureThreshold: 5

        # CSI sidecar container for provisioning
        - name: csi-provisioner
          image: {{ .Values.controllerServer.provisioner.image }}
          imagePullPolicy: {{ .Values.controllerServer.provisioner.pullPolicy }}
          args:
            - "--v={{ .Values.controllerServer.provisioner.logLevel }}"
            - "--csi-address=$(ADDRESS)"
            - "--http-endpoint=:8080"
            - "--timeout={{ .Values.controllerServer.provisioner.timeout }}"
            - "--worker-threads={{ .Values.controllerServer.provisioner.workerThreads }}"
            - "--retry-interval-start={{ .Values.controllerServer.provisioner.retryIntervalStart }}"
            {{- if gt (int .Values.controllerServer.replicaCount) 1 }}
            - "--leader-election"
            {{- end }}
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          ports:
            - containerPort: 8080
              name: http-endpoint
              protocol: TCP
          {{- if .Values.controllerServer.provisioner.resources }}

          # Resource requests and limits for the driver provisioner sidecar
          # Adjust as necessary based on your cluster capacity and requirements
          # Limits should be set to prevent excessive resource consumption
          # Requests should be set to ensure the container gets scheduled
          resources:
            {{- toYaml .Values.controllerServer.provisioner.resources | nindent 12 }}
          {{- end }}
          {{- if gt (int .Values.controllerServer.replicaCount) 1 }}

          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz/leader-election
              port: http-endpoint
            initialDelaySeconds: 10
            timeoutSeconds: 10
            periodSeconds: 20
          {{- end }}

          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy

        # CSI sidecar container for attach/detach operations
        - name: csi-attacher
          image: {{ .Values.controllerServer.attacher.image }}
          imagePullPolicy: {{ .Values.controllerServer.attacher.pullPolicy }}
          args:
            - "--v={{ .Values.controllerServer.attacher.logLevel }}"
            - "--csi-address=$(ADDRESS)"
            - "--http-endpoint=:8081"
            - "--timeout={{ .Values.controllerServer.attacher.timeout }}"
            {{- if gt (int .Values.controllerServer.replicaCount) 1 }}
            - "--leader-election"
            {{- end }}
          env:
            - name: MY_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          ports:
            - containerPort: 8081
              name: http-endpoint
              protocol: TCP
          {{- if .Values.controllerServer.attacher.resources }}

          # Resource requests and limits for the driver attacher sidecar
          # Adjust as necessary based on your cluster capacity and requirements
          # Limits should be set to prevent excessive resource consumption
          # Requests should be set to ensure the container gets scheduled
          resources:
            {{- toYaml .Values.controllerServer.attacher.resources | nindent 12 }}
          {{- end }}
          {{- if gt (int .Values.controllerServer.replicaCount) 1 }}

          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz/leader-election
              port: http-endpoint
            initialDelaySeconds: 10
            timeoutSeconds: 10
            periodSeconds: 20
          {{- end }}

          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy

        # CSI sidecar container for volume resizing
        - name: csi-resizer
          image: {{ .Values.controllerServer.resizer.image }}
          imagePullPolicy: {{ .Values.controllerServer.resizer.pullPolicy }}
          args:
            - "--v={{ .Values.controllerServer.resizer.logLevel }}"
            - "--csi-address=$(ADDRESS)"
            - "--http-endpoint=:8082"
            - "--timeout={{ .Values.controllerServer.resizer.timeout }}"
            {{- if gt (int .Values.controllerServer.replicaCount) 1 }}
            - "--leader-election"
            {{- end }}
          env:
            - name: ADDRESS
              value: /var/lib/csi/sockets/pluginproxy/csi.sock
          ports:
            - containerPort: 8082
              name: http-endpoint
              protocol: TCP
          {{- if .Values.controllerServer.resizer.resources }}

          # Resource requests and limits for the driver resizer sidecar
          # Adjust as necessary based on your cluster capacity and requirements
          # Limits should be set to prevent excessive resource consumption
          # Requests should be set to ensure the container gets scheduled
          resources:
            {{- toYaml .Values.controllerServer.resizer.resources | nindent 12 }}
          {{- end }}
          {{- if gt (int .Values.controllerServer.replicaCount) 1 }}

          livenessProbe:
            failureThreshold: 5
            httpGet:
              path: /healthz/leader-election
              port: http-endpoint
            initialDelaySeconds: 10
            timeoutSeconds: 10
            periodSeconds: 20
          {{- end }}

          volumeMounts:
            - name: socket-dir
              mountPath: /var/lib/csi/sockets/pluginproxy

      volumes:
        # CSI socket shared between containers
        - name: socket-dir
          emptyDir: {}